{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOa3oNXeFBjU",
        "outputId": "05b5375c-dd75-4d61-b478-5473fdedf21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/hw3_rl')"
      ],
      "metadata": {
        "id": "_BvXDzWFFGMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/My\\ Drive/hw3_rl/gridworld_hw3_q1.py /content/\n",
        "!cp /content/drive/My\\ Drive/hw3_rl/gw55_hw3.py /content/\n",
        "!cp /content/drive/My\\ Drive/hw3_rl/gw55_hw3_q1.py /content/\n",
        "!cp /content/drive/My\\ Drive/hw3_rl/utils.py /content/"
      ],
      "metadata": {
        "id": "dYh0WSZDFIc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gridworld_hw3_q1 import GridWorld\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "#From gw55_hw3\n",
        "def GridWorld5x5(p=0.9):\n",
        "    rewards = {\n",
        "        (2,0): -100,\n",
        "        (4,4):  100\n",
        "    }\n",
        "    walls = [(2,2), (2,3), (2,4), (3,2)]\n",
        "\n",
        "    # deterministic transition table\n",
        "    T = {\n",
        "        (0,0): { 'R': (0,1), 'D': (1,0) },\n",
        "        (0,1): { 'R': (0,2), 'L': (0,0), 'D': (1,1) },\n",
        "        (0,2): { 'R': (0,3), 'L': (0,1), 'D': (1,2) },\n",
        "        (0,3): { 'R': (0,4), 'L': (0,2), 'D': (1,3) },\n",
        "        (0,4): { 'L': (0,3), 'D': (1,4) },\n",
        "\n",
        "        (1,0): { 'R': (1,1), 'D': (2,0), 'U': (0,0) },\n",
        "        (1,1): { 'R': (1,2), 'L': (1,0), 'D': (2,1), 'U': (0,1) },\n",
        "        (1,2): { 'R': (1,3), 'L': (1,1), 'U': (0,2) },\n",
        "        (1,3): { 'R': (1,4), 'L': (1,2), 'U': (0,3) },\n",
        "        (1,4): { 'L': (1,3), 'U': (0,4) },\n",
        "\n",
        "        (2,0): { 'R': (2,1), 'D': (3,0), 'U': (1,0) },\n",
        "        (2,1): { 'L': (2,0), 'D': (3,1), 'U': (1,1) },\n",
        "\n",
        "        (3,0): { 'R': (3,1), 'D': (4,0), 'U': (2,0) },\n",
        "        (3,1): { 'L': (3,0), 'D': (4,1), 'U': (2,1) },\n",
        "        (3,3): { 'R': (3,4), 'D': (4,3) },\n",
        "        (3,4): { 'L': (3,3), 'D': (4,4) },\n",
        "\n",
        "        (4,0): { 'R': (4,1), 'U': (3,0) },\n",
        "        (4,1): { 'R': (4,2), 'L': (4,0), 'U': (3,1) },\n",
        "        (4,2): { 'R': (4,3), 'L': (4,1) },\n",
        "        (4,3): { 'R': (4,4), 'L': (4,2), 'U': (3,3) },\n",
        "        (4,4): { 'L': (4,3), 'U': (3,4) },\n",
        "    }\n",
        "\n",
        "    for s in T:\n",
        "        ns_l = list(T[s].values())\n",
        "        for a in T[s]:\n",
        "            ns = T[s][a] #next state\n",
        "            rs = ns_l[np.random.choice(len(ns_l))] #random\n",
        "            if ns == rs:\n",
        "                T[s][a] = { ns: 1.0 }\n",
        "            else:\n",
        "                T[s][a] = { ns: p, rs: np.round(1-p,2) }\n",
        "\n",
        "    g = GridWorld(5, 5, start_position=(0, 0),\n",
        "            pass_through_reward=0, rewards=rewards, walls = walls)\n",
        "    g.probs = T\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "FqjQbrRhFJT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gridworld_hw3_q1 import GridWorld\n",
        "#from gw55_hw3 import *\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "# reward shaping global constants\n",
        "REWARD_BONUS = 4          # Bonus for moving closer to the goal\n",
        "PENALTY = -1              # Penalty for moving further away\n",
        "OSCILLATION_PENALTY = -6   # Penalty for oscillatory moves\n",
        "GOAL_STATE = (4, 4)\n",
        "\n",
        "def manhattan_distance(state, goal):\n",
        "    #distance from current position to goal\n",
        "    return abs(state[0] - goal[0]) + abs(state[1] - goal[1])\n",
        "\n",
        "def get_states(g):\n",
        "    augmented = []\n",
        "    for s in g.all_states():\n",
        "        # Initial augmented state when no previous state exists\n",
        "        augmented.append((s, None))\n",
        "        for s_prev in g.all_states():\n",
        "            augmented.append((s, s_prev))\n",
        "    return augmented\n",
        "\n",
        "def value_iteration(g, gamma=0.9, theta=1e-4, max_iterations=1000):\n",
        "    augmented_states = get_states(g)\n",
        "    # Initialize value function\n",
        "    V = {state: 0 for state in augmented_states}\n",
        "    iteration = 0\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        iteration += 1\n",
        "        for (s, last) in augmented_states:\n",
        "            # If s is terminal, skip update\n",
        "            if s in g.rewards:\n",
        "                continue\n",
        "\n",
        "            best_value = float('-inf')\n",
        "            possible_actions = g.actions(s)\n",
        "\n",
        "            # V(s) equation for each action\n",
        "            for a in possible_actions:\n",
        "                value = 0\n",
        "                next_states_probs = g.probs.get(s, {}).get(a, {})\n",
        "                for s_next, prob in next_states_probs.items():\n",
        "\n",
        "                    new_state = (s_next, s)\n",
        "                    base_reward = g.world[s_next]\n",
        "\n",
        "                    # Apply oscillation penalty if s_next equals the last state\n",
        "                    extra_penalty = OSCILLATION_PENALTY if (last is not None and s_next == last) else 0\n",
        "                    r = base_reward + extra_penalty\n",
        "                    value += prob * (r + gamma * V.get(new_state, 0))\n",
        "                best_value = max(best_value, value)\n",
        "            delta = max(delta, abs(best_value - V[(s, last)]))\n",
        "            V[(s, last)] = best_value\n",
        "        if delta < theta or iteration >= max_iterations:\n",
        "            break\n",
        "\n",
        "    # Extract optimal policy based on augmented states and value function\n",
        "    policy = {}\n",
        "    for (s, last) in augmented_states:\n",
        "        if s in g.rewards:\n",
        "            continue      #Skip our terminal states\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "        possible_actions = g.actions(s)\n",
        "        for a in possible_actions:\n",
        "            value = 0\n",
        "            next_states_probs = g.probs.get(s, {}).get(a, {})\n",
        "\n",
        "            # Applies the oscillation penalty to discourage staying in the same place\n",
        "            for s_next, prob in next_states_probs.items():\n",
        "                new_state = (s_next, s)\n",
        "                base_reward = g.world[s_next]\n",
        "                extra_penalty = OSCILLATION_PENALTY if (last is not None and s_next == last) else 0\n",
        "                r = base_reward + extra_penalty\n",
        "                value += prob * (r + gamma * V.get(new_state, 0))\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = a\n",
        "        policy[(s, last)] = best_action\n",
        "\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "Y0DO0GiCJ_mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    g = GridWorld5x5()\n",
        "    policy, V = value_iteration(g,gamma=0.9)\n",
        "\n",
        "    # Initialize augmented state: starting at g.start_position with no last state\n",
        "    current_augmented_state = (g.start_position, None)\n",
        "\n",
        "    while not g.game_over():\n",
        "        g.print()\n",
        "        print()\n",
        "\n",
        "        current_state, last_state = current_augmented_state\n",
        "        actions = g.actions(current_state)\n",
        "        # Choose action from policy for the augmented state\n",
        "        action = policy.get(current_augmented_state, random.choice(actions))\n",
        "        print(f\"Chosen action: {action}\")\n",
        "\n",
        "        next_state, reward = g.move(action)\n",
        "\n",
        "        # Compute Manhattan distances for reward shaping\n",
        "        md_current = manhattan_distance(current_state, GOAL_STATE)\n",
        "        md_next = manhattan_distance(next_state, GOAL_STATE)\n",
        "        if md_next < md_current:\n",
        "            reward += REWARD_BONUS\n",
        "            print(f\"Reward bonus for moving closer: {REWARD_BONUS}\")\n",
        "        elif md_next > md_current:\n",
        "            reward += PENALTY\n",
        "            print(f\"Penalty for moving further: {PENALTY}\")\n",
        "\n",
        "\n",
        "        print(f\"Moved to state: {next_state}\")\n",
        "        print(f\"Reward: {reward}\")\n",
        "\n",
        "        # Update the agent's state\n",
        "        g.set_state(next_state)\n",
        "        current_augmented_state = (next_state, current_state)\n",
        "\n",
        "    # Print the optimal augmented policy and value function\n",
        "    print(\"Optimal Policy:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            key = ((r, c), None)\n",
        "            if key in policy:\n",
        "                print(f\"{(r, c)}: {policy[key]}\", end=\", \")\n",
        "        print()\n",
        "\n",
        "    print(\"Final Value Function:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            key = ((r, c), None)\n",
        "            print(f\"{(r, c)}: {V.get(key, 0):.2f}\", end=\", \")\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS9vMNasRbjg",
        "outputId": "88e5584f-841c-4c91-be05-71bd26861106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (0, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (1, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (2, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B|o|x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (3, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| |o|x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (4, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (4, 2)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | |o| |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (4, 3)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o|G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (4, 4)\n",
            "Reward: 104.0\n",
            "Optimal Policy:\n",
            "(0, 0): R, (0, 1): D, (0, 2): D, (0, 3): L, (0, 4): D, \n",
            "(1, 0): R, (1, 1): D, (1, 2): L, (1, 3): L, (1, 4): L, \n",
            "(2, 1): D, \n",
            "(3, 0): R, (3, 1): D, (3, 3): D, (3, 4): D, \n",
            "(4, 0): R, (4, 1): R, (4, 2): R, (4, 3): R, \n",
            "Final Value Function:\n",
            "(0, 0): 30.67, (0, 1): 34.09, (0, 2): 30.59, (0, 3): 26.93, (0, 4): 23.91, \n",
            "(1, 0): 34.58, (1, 1): 39.35, (1, 2): 34.05, (1, 3): 29.52, (1, 4): 26.56, \n",
            "(2, 0): 0.00, (2, 1): 45.40, (2, 2): 0.00, (2, 3): 0.00, (2, 4): 0.00, \n",
            "(3, 0): 61.01, (3, 1): 68.39, (3, 2): 0.00, (3, 3): 87.56, (3, 4): 97.88, \n",
            "(4, 0): 70.49, (4, 1): 78.32, (4, 2): 87.02, (4, 3): 97.29, (4, 4): 0.00, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    g = GridWorld5x5(p=0.9)\n",
        "    policy, V = value_iteration(g, gamma=0.3)\n",
        "\n",
        "    # Initialize augmented state: starting at g.start_position with no last state\n",
        "    current_augmented_state = (g.start_position, None)\n",
        "\n",
        "    while not g.game_over():\n",
        "        g.print()\n",
        "        print()\n",
        "\n",
        "        current_state, last_state = current_augmented_state\n",
        "        actions = g.actions(current_state)\n",
        "        # Choose action from policy for the augmented state\n",
        "        action = policy.get(current_augmented_state, random.choice(actions))\n",
        "        print(f\"Chosen action: {action}\")\n",
        "\n",
        "        next_state, reward = g.move(action)\n",
        "\n",
        "        # Compute Manhattan distances for reward shaping\n",
        "        md_current = manhattan_distance(current_state, GOAL_STATE)\n",
        "        md_next = manhattan_distance(next_state, GOAL_STATE)\n",
        "        if md_next < md_current:\n",
        "            reward += REWARD_BONUS\n",
        "            print(f\"Reward bonus for moving closer: {REWARD_BONUS}\")\n",
        "        elif md_next > md_current:\n",
        "            reward += PENALTY\n",
        "            print(f\"Penalty for moving further: {PENALTY}\")\n",
        "\n",
        "        # (Oscillation penalty is already integrated in value_iteration)\n",
        "\n",
        "        print(f\"Moved to state: {next_state}\")\n",
        "        print(f\"Reward: {reward}\")\n",
        "\n",
        "        # Update the agent's state: new augmented state becomes (next_state, current_state)\n",
        "        g.set_state(next_state)\n",
        "        current_augmented_state = (next_state, current_state)\n",
        "\n",
        "    # Optionally print the optimal augmented policy and value function\n",
        "    print(\"Optimal Policy:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            key = ((r, c), None)\n",
        "            if key in policy:\n",
        "                print(f\"{(r, c)}: {policy[key]}\", end=\", \")\n",
        "        print()\n",
        "\n",
        "    print(\"Final Value Function:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            key = ((r, c), None)\n",
        "            print(f\"{(r, c)}: {V.get(key, 0):.2f}\", end=\", \")\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLrdv8MyQVY8",
        "outputId": "bd61c8bd-de38-46b6-dadb-6e0fa836b29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (1, 0)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (1, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (2, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B|o|x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (3, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| |o|x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (4, 1)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (4, 2)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | |o| |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (4, 3)\n",
            "Reward: 4.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o|G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Reward bonus for moving closer: 4\n",
            "Moved to state: (4, 4)\n",
            "Reward: 104.0\n",
            "Optimal Policy:\n",
            "(0, 0): D, (0, 1): D, (0, 2): L, (0, 3): L, (0, 4): L, \n",
            "(1, 0): R, (1, 1): D, (1, 2): U, (1, 3): L, (1, 4): L, \n",
            "(2, 1): D, \n",
            "(3, 0): D, (3, 1): D, (3, 3): D, (3, 4): D, \n",
            "(4, 0): R, (4, 1): R, (4, 2): R, (4, 3): R, \n",
            "Final Value Function:\n",
            "(0, 0): 0.01, (0, 1): 0.02, (0, 2): 0.00, (0, 3): -0.00, (0, 4): -0.02, \n",
            "(1, 0): 0.04, (1, 1): 0.13, (1, 2): 0.00, (1, 3): 0.00, (1, 4): -0.02, \n",
            "(2, 0): 0.00, (2, 1): 0.54, (2, 2): 0.00, (2, 3): 0.00, (2, 4): 0.00, \n",
            "(3, 0): 0.40, (3, 1): 1.80, (3, 2): 0.00, (3, 3): 27.18, (3, 4): 90.80, \n",
            "(4, 0): 1.96, (4, 1): 7.28, (4, 2): 27.01, (4, 3): 90.63, (4, 4): 0.00, \n"
          ]
        }
      ]
    }
  ]
}